{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>**Welcome to the Summarization Notebook.**</h3></center>\n",
    "\n",
    "In this assignment, you are going to train a neural network to summarize news articles.\n",
    "Your neural network is going to learn from example, as we provide you with (article, summary) pairs.\n",
    "We provide you with a **toy dataset** made of only articles about police related news.\n",
    "Usual datasets can be 20x larger in size, but we have reduced it for computational purposes.\n",
    "\n",
    "You will do this using a Transformer network, from the __[Attention is all you need](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)__ paper.\n",
    "In this assignment you will:\n",
    "- Learn to process text into sub-word tokens, to avoid fixed vocabulary sizes, and UNK tokens.\n",
    "- Implement the key conceptual blocks of a Transformer.\n",
    "- Use a Transformer to read a news article, and produce a summary.\n",
    "- Perform operations on learned word-vectors to examine what the model has learned.\n",
    "\n",
    "    \n",
    "** Before you start **\n",
    "\n",
    "You should read the Attention is all you need paper.\n",
    "We are providing you with skeleton code for the Transformer, but there will have to implement 5 conceptual blocks of the transformer yourself:\n",
    "-  AttentionQKV: the Query, Key, Value attention mechanism at the center of the Transformer\n",
    "- MultiHeadAttention: the multiple heads that enable each input to attend at many places at once.\n",
    "- PositionEmbedding: the sinusoid-based position embedding of the Transformer.\n",
    "- Encoder & Decoder: The encoder (that reads inputs, such as news articles), the decoder (that produces the output summary, one token at a time)\n",
    "- Full Transformer: piecing it all together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import GPT\n",
    "import sentencepiece as spm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import capita\n",
    "\n",
    "root_folder = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word piece model that will be used to tokenize the texts into\n",
    "# word pieces with a vocabulary size of 10000\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(root_folder+\"dataset/wp_vocab10000.model\")\n",
    "\n",
    "vocab = [line.split('\\t')[0] for line in open(root_folder+\"dataset/wp_vocab10000.vocab\", \"r\")]\n",
    "pad_index = vocab.index('#')\n",
    "\n",
    "def pad_sequence(numerized, pad_index, to_length):\n",
    "    pad = numerized[:to_length]\n",
    "    padded = pad + [pad_index] * (to_length - len(pad))\n",
    "    mask = [w != pad_index for w in padded]\n",
    "    return padded, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the blocks of the Transformer are implemented, we can create a full model with placeholders and a loss.\n",
    "\n",
    "We've helped you with the placeholders, and the loss, as it is similar to the one in the previous assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are giving you the trainer, as it is similar to the one\n",
    "# you created in the Language Modeling assignment.\n",
    "\n",
    "class GPTTrainer():\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, output_length, n_layers, d_filter, learning_rate=1e-3):\n",
    "\n",
    "        self.target_sequence = tf.placeholder(tf.int32, shape=(None,output_length),name=\"target_sequence\")\n",
    "        self.decoder_mask = tf.placeholder(tf.bool, shape=(None,output_length),name=\"decoder_mask\")\n",
    "\n",
    "        self.model = GPT(vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, d_filter=d_filter)\n",
    "\n",
    "        self.decoded_logits = self.model(self.target_sequence, decoder_mask=self.decoder_mask)\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        \n",
    "        # Summarization loss\n",
    "        self.loss = tf.losses.sparse_softmax_cross_entropy(self.target_sequence, self.decoded_logits, tf.cast(self.decoder_mask, tf.float32))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "        self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now instantiate the Transformer with our sets of hyperparameters specific to the task of summarization.\n",
    "In summarization, we are going to go from documents with up to 400 words, to documents with up to 100 words.\n",
    "The vocabulary size is set for you, and is of 10,000 words (we are using WordPieces, [here is a paper about subword encoding](http://aclweb.org/anthology/P18-1007), if you are interested)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset related parameters\n",
    "vocab_size = len(vocab)\n",
    "ilength = 400 # Length of the article\n",
    "olength  = 100 # Length of the summaries\n",
    "\n",
    "# Model related parameters, feel free to modify these.\n",
    "n_layers = 12\n",
    "d_model  = 104\n",
    "d_filter = 416\n",
    "\n",
    "model = GPTTrainer(vocab_size, d_model, ilength, n_layers, d_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your objective is to train the Language on the dataset you are provided to reach a **validation loss <= 4.50**\n",
    "\n",
    "Careful: we will be testing this loss on an unreleased test set, so make sure to evaluate properly on a validation set and not overfit.\n",
    "\n",
    "You must save the model you want us to test under: models/final_transformer_summarization (the .index, .meta and .data files)\n",
    "\n",
    "**Advice**:\n",
    "- It should be possible to attain validation loss <= 4.50 with the model dimensions we've specified (n_layers=6, d_model=104, d_filter=416), but you can tune these hyperparameters. Increasing d_model will yield better model, at the cost of longer training time.\n",
    "- You should try tuning the learning rate, as well as what optimizer you use.\n",
    "- You might need to train for a few (up to 2 hours) to obtain our expected loss. Remember to tune your hyperparameters first, once you find ones that work well, let it train for longer.\n",
    "\n",
    "**Dataset**: as in the previous notebook, make sure the dataset files are in the `dataset` folder. These can be found on the Google Drive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61055, 1558)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(root_folder+\"dataset/summarization_dataset_preprocessed.json\", \"r\") as f:\n",
    "\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# We load the dataset, and split it into 2 sub-datasets based on if they are training or validation.\n",
    "# Feel free to split this dataset another way, but remember, a validation set is important, to have an idea of \n",
    "# the amount of overfitting that has occurred!\n",
    "\n",
    "d_train = [d for d in dataset if d['cut'] == 'training']\n",
    "d_valid = [d for d in dataset if d['cut'] == 'evaluation']\n",
    "\n",
    "len(d_train), len(d_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tbilisi, Georgia (CNN)Police have shot and killed a white tiger that killed a man Wednesday in Tbilisi, Georgia, a Ministry of Internal Affairs representative said, after severe flooding allowed hundreds of wild animals to escape the city zoo. \n",
      "The tiger attack happened at a warehouse in the city center. The animal had been unaccounted for since the weekend floods destroyed the zoo premises.\n",
      "The man killed, who was 43, worked in a company based in the warehouse, the Ministry of Internal Affairs said. Doctors said he was attacked in the throat and died before reaching the hospital. \n",
      "Experts are still searching the warehouse, the ministry said, adding that earlier reports that the tiger had injured a second man were unfounded. \n",
      "The zoo administration said Wednesday that another tiger was still missing. It was unable to confirm if the creature was dead or had escaped alive.\n",
      "Georgian Prime Minister Irakli Garibashvili apologized to the public, saying he had been misinformed by the zoo's management when he'd previously said there were no more dangerous animals on the run.\n",
      "City residents were urged to stay indoors for their own safety in the immediate aftermath of the floods. Volunteers have since been helping city workers with the cleanup operation.\n",
      "At least 19 people died in the flooding, according to Civil Georgia, a news website run by the nongovernmental organization United Nations Association of Georgia. Six more remained missing, it said Tuesday, citing the State Security and Crisis Management Council.\n",
      "Meanwhile, the zoo lost about half of its 600 animals, including lions, tigers, bears and wolves, in the natural disaster. \n",
      "Some animals have since been recaptured, Civil Georgia reported. Others died in the floods or have been killed by police as they scour the streets for escapees.\n",
      "Russian state news outlet RT.com  that an African penguin had made it 60 kilometers (37 miles) downriver from Tbilisi before being caught alive in a dragnet on the border with Azerbaijan. \n",
      "Video from the city showed a large crocodile being restrained by rescuers, as well as a hippopotamus standing in floodwaters, looking confused.\n",
      "The latter was eventually cornered in a city square before being tranquilized and recaptured.\n",
      "One terrified bear escaped the flood by perching on a window ledge.\n",
      "Video footage also showed devastation across swaths of the Georgian capital, where flash floods swept away roads, at least one house and many trees. The corpses of dead animals could be seen amid the wreckage.\n",
      "The problems began before midnight Saturday when heavy rainfall turned the Vere River, usually little more than a stream through the center of Tbilisi, into a raging torrent, according to Civil Georgia.\n",
      "Images on Tbilisi City Hall's Facebook page showed roads washed out, hillsides collapsed and vehicles tossed about like toys. Rescue workers carried people on their shoulders through waist-high water.\n",
      "Garibashvili extended his condolences Tuesday to the families of those killed in the flooding.\n",
      "He also proposed the creation of a park in the zoo premises to honor those lost. \"It will be a park of solidarity, a symbol of our unity, selflessness, and mutual support,\" he said in a statement on his website.\n",
      "President Georgi Margvelashvili earlier said the capital's mayoral office would help those who had lost out financially as a result of the floods.\n",
      "\"The situation is difficult, but it can be handled except for the fact that we cannot bring back those who died,\" he said.\n",
      "According to the World Wildlife Fund, as few as 3,200 tigers exist in the wild today.\n",
      "\n",
      "Journalist Eka Kadagishvili reported from Tbilisi, and Laura Smith-Spark wrote from London. CNN's Kimberly Hutcherson contributed to this report.\n",
      "=======================\n",
      "=======================\n",
      "Police have shot dead a tiger that killed a man in Tbilisi, Georgia, a government official says, after zoo animals escaped in weekend flooding.\n"
     ]
    }
   ],
   "source": [
    "# An example (article, summary) pair in the training data:\n",
    "\n",
    "print(d_train[145]['story'])\n",
    "print(\"=======================\\n=======================\")\n",
    "print(d_train[145]['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the previous assignment, we create a function to get a random batch to train on, given a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(dataset, batch_size):\n",
    "    indices = list(np.random.randint(0, len(dataset), size=batch_size))\n",
    "    \n",
    "    batch = [dataset[i] for i in indices]\n",
    "    batch_output = np.array([a['input'] for a in batch])\n",
    "    batch_output_mask = np.array([a['input_mask'] for a in batch])\n",
    "    \n",
    "    return batch_output, batch_output_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iteration 0, Train Loss: 13.942840576171875, Val Loss: 12.402144432067871\n",
      "Epoch 0 Iteration 50, Train Loss: 8.218381881713867, Val Loss: 8.214381217956543\n"
     ]
    }
   ],
   "source": [
    "# Skeleton code, as in the previous notebook.\n",
    "# Write code training code and save your best performing model on the\n",
    "# validation set. We will be testing the loss on a held-out test dataset.\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # This is how you randomly initialize the Transformer weights.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    epochs = 20 #previously 50\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        batch_size = 128\n",
    "        iterations = len(d_train) // batch_size\n",
    "        \n",
    "        # build validation set\n",
    "        e_output, e_output_mask = build_batch(d_valid, 200)\n",
    "        \n",
    "        for iteration in range(iterations):       \n",
    "\n",
    "            # Create a random mini-batch from the training dataset\n",
    "            batch_output, batch_output_mask = build_batch(d_train, batch_size)\n",
    "            # Build the feed-dict connecting placeholders and mini-batch\n",
    "            feed = {model.target_sequence: batch_output, model.decoder_mask: batch_output_mask}\n",
    "\n",
    "            # Obtain the loss. Be careful when you use the train_op and not, as previously.\n",
    "            train_loss, _, step = sess.run([model.loss, model.train_op, model.global_step], feed_dict=feed)\n",
    "            \n",
    "            if iteration % 50 == 0:\n",
    "                \n",
    "                # get validation loss\n",
    "                feed_val = {model.target_sequence: e_output, model.decoder_mask: e_output_mask}\n",
    "                valid_loss = sess.run(model.loss, feed_dict=feed_val)\n",
    "                \n",
    "                print(\"Epoch {} Iteration {}, Train Loss: {}, Val Loss: {}\".format(epoch, iteration, train_loss, valid_loss))\n",
    "            \n",
    "                \n",
    "#                 print(\"Epoch {} Iteration {}, Train Loss: {}\".format(epoch, iteration, train_loss))\n",
    "                \n",
    "#                 This is how you save model weights into a file\n",
    "#                 model.saver.save(sess, root_folder+\"models/gpt_test\")    \n",
    "\n",
    "#                 # This is how you restore a model previously saved\n",
    "#                 model.saver.restore(sess, root_folder+\"models/transformer_summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Summarization model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have trained a Transformer to perform Summarization, we will use the model on news articles from the wild.\n",
    "\n",
    "The three subsections below explore what the model has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the file path to your best performing model in the string below.\n",
    "\n",
    "model_file = root_folder+\"models/gpt_test\"\n",
    "# model_file = root_folder+\"models/transformer_summarizer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the validation loss of your model. This part could be used, as in our previous notebook, in deciding what is a likely, vs. unlikely summary for an article.\n",
    "\n",
    "We will use the code here with the unreleased test-set to evaluate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/gpt_test\n",
      "Validation loss: 6.28927\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    model.saver.restore(sess, model_file)\n",
    "\n",
    "    e_output, e_output_mask = build_batch(d_valid, 200)\n",
    "    feed = {model.target_sequence: e_output, model.decoder_mask: e_output_mask}\n",
    "    valid_loss = sess.run(model.loss, feed_dict=feed)\n",
    "    print(\"Validation loss:\", valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model we have built is meant to be used to generate summaries for new articles we do not have summaries for.\n",
    "We got a [news article](https://www.chicagotribune.com/news/local/breaking/ct-met-officer-shot-20190309-story.html) from the Chicago Tribune about a police shooting, and want to use our model to produce a summary.\n",
    "\n",
    "As you will see, our model is still limited in its ability, and will most likely not produce a perfect summary, however, with more data and training, this model would be able to produce good summaries.\n",
    "The article you produce should look like broken English sentences, but should roughly correspond to the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/gpt_test\n",
      "The final summary:\n",
      "<unk>  ↑↑ the  ↑↑ prince  ↑↑ eddie prince  ⇧⇧ lrb lrb incidents -  ⇧rrb cnn lrb - the  ↑↑ police  ↑↑ division division of of  ↑↑ the :  ↑↑ police police department department  ↑↑ county told  ↑↑ police the department  ↑↑ police told department  ↑↑ the the  ↑↑ : :  ↑↑ the the  ↑↑ police the department  ↑↑ police :  ↑↑ the the  ↑↑ supreme  ↑↑ police police department department . ,  the↑  the↑  the↑  the↑  ↑↑ supreme  ↑↑ police  department↑ . the  ↑↑ the the  ↑↑ the  ↑↑  khkhss  .↑  the↑  the↑  the↑  the↑  the↑  the↑  the↑  the↑  the↑  the↑  the↑  the↑  the↑  the↑  the↑  the↑  the↑  the↑  the↑  the↑  the↑  the↑  the↑  ↑↑ the  ↑↑  ↑↑  ↑↑ the the  ↑↑ the  ↑↑  ↑↑  ↑↑ the  ↑↑ the  ↑↑ the the  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑ the\n"
     ]
    }
   ],
   "source": [
    "output_length = 400\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model.saver.restore(sess, model_file)\n",
    "\n",
    "    decoded_so_far = [0]\n",
    "    \n",
    "    for j in range(output_length):\n",
    "        padded_decoder_input, decoder_mask = pad_sequence(decoded_so_far, pad_index, output_length)\n",
    "        padded_decoder_input = [padded_decoder_input]\n",
    "        decoder_mask = [decoder_mask]\n",
    "#         print(\"========================\")\n",
    "#         print(padded_decoder_input)\n",
    "        # Use the model to find the distrbution over the vocabulary for the next word\n",
    "        feed = {model.target_sequence: padded_decoder_input,\n",
    "                model.decoder_mask: decoder_mask}\n",
    "        logits = sess.run([model.decoded_logits], feed_dict=feed)\n",
    "    \n",
    "        chosen_words = np.argmax(logits[0], axis=2) # Take the argmax, getting the most likely next word\n",
    "        decoded_so_far.append(int(chosen_words[0, j])) # We add it to the summary so far\n",
    "\n",
    "\n",
    "print(\"The final summary:\")\n",
    "print(\"\".join([vocab[i] for i in decoded_so_far]).replace(\"▁\", \" \"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we train learns word representations for each word in our vocabulary. A word represention is a vector of **dim** size.\n",
    "\n",
    "It is common in NLP to inspect the word vectors, as some properties of language often appear in the embedding structure.\n",
    "\n",
    "\n",
    "We are going to load the word embeddings learned by our model, and inspect it.\n",
    "Because our network was not trained for long, we are going for the simplest patterns, but if we let the network train longer, it learns more complex, semantic patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronouns serve very similar purposes, therefore we should expect the representation of \"he\" and \"she\" to be similar, and have cosine similarity.\n",
    "\n",
    "- **TODO**:  Find the cosine similarity between the vectors that represent words \"she\" and \"he\".\n",
    "- **TODO**:  Find the cosine similarity between the vectors that represent words \"more\" and \"less\".\n",
    "\n",
    "We can contrast that with the cosine similarity to a random, non-related word, like \"ball\", or \"gorilla\".\n",
    "- **TODO**: Compute the cosine similarity between \"she\" and \"ball\".\n",
    "- **TODO**: Compute the cosine similarity between \"more\" and \"protest\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These effects are unfortunately small, as we have only trained the network on a few hours on a few thousand articles.\n",
    "However, the same model trained for longer on more data exhibits many interesting semantic and syntactic patterns, such as:\n",
    "\n",
    "- Words vectors with high cosine similarity usually represent words that have semantic similarity (such as duck and pigeon)\n",
    "- Analogies can occur, a famous case is that of: woman - man + king ≈ queen. Or france - paris + rome ≈ italy.\n",
    "\n",
    "- Looking at top-k similar words can help find synonyms.\n",
    "\n",
    "To read examples of more complex patterns that appear in word embedding spaces, read [this blog](https://explosion.ai/blog/sense2vec-with-spacy). To play with a live demo and try similarities on rich word embeddings, [go here.](https://explosion.ai/demos/sense2vec)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
