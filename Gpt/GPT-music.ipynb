{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do:\n",
    "\n",
    "1. Data\n",
    "    1. Use more music files from other artist\n",
    "    2. Don't just concatenate all the text files together, make sure difference pieces of music are properly separated\n",
    "    3. Batch the sequences in a better way (not just [1,2,3], [4,5,6] but [1,2,3], [2,3,4], [3,4,5], [5,6,7]\n",
    "    \n",
    "    \n",
    "2. Model\n",
    "    1. Choose from top k tokens, not just the highest probability one (DONE)\n",
    "    2. Try optimizing the top k words selection simulating the j-th element.\n",
    "    2. Try using less hops\n",
    "    3. Write the music decoder to auto decode the music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>**Music generation using GPT architecture**</h3></center>\n",
    "\n",
    "We are using a Transformer network, from the __[Attention is all you need](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)__ paper.\n",
    "We are going to:\n",
    "- Process music sheet into sub-word tokens, to avoid fixed vocabulary sizes, and UNK tokens.\n",
    "- Implement the key conceptual blocks of a Transformer.\n",
    "- Train a Transformer to generate new tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import GPT\n",
    "import sentencepiece as spm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import capita\n",
    "\n",
    "root_folder = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word piece model that will be used to tokenize the texts into\n",
    "# word pieces with a vocabulary size of 10000\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(root_folder+\"dataset/wp_vocab10000.model\")\n",
    "\n",
    "vocab = [line.split('\\t')[0] for line in open(root_folder+\"dataset/wp_vocab10000.vocab\", \"r\")]\n",
    "pad_index = vocab.index('#')\n",
    "\n",
    "def pad_sequence(numerized, pad_index, to_length):\n",
    "    pad = numerized[:to_length]\n",
    "    padded = pad + [pad_index] * (to_length - len(pad))\n",
    "    mask = [w != pad_index for w in padded]\n",
    "    return padded, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the blocks of the Transformer are implemented, we can create a full model with placeholders and a loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTTrainer():\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, output_length, n_layers, d_filter, learning_rate=1e-3):\n",
    "\n",
    "        self.target_sequence = tf.placeholder(tf.int32, shape=(None,output_length),name=\"target_sequence\")\n",
    "        self.decoder_mask = tf.placeholder(tf.bool, shape=(None,output_length),name=\"decoder_mask\")\n",
    "\n",
    "        self.model = GPT(vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, d_filter=d_filter)\n",
    "\n",
    "        self.decoded_logits = self.model(self.target_sequence, decoder_mask=self.decoder_mask)\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        \n",
    "        # Summarization loss\n",
    "        self.loss = tf.losses.sparse_softmax_cross_entropy(self.target_sequence, self.decoded_logits, tf.cast(self.decoder_mask, tf.float32))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "        self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading music data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset related parameters\n",
    "vocab_size = len(vocab)\n",
    "ilength = 128 # Length of the article (Previously 400)\n",
    "olength  = 100 # Length of the summaries\n",
    "\n",
    "# Model related parameters, feel free to modify these.\n",
    "n_layers = 6  # Originally 12\n",
    "d_model  = 104\n",
    "d_filter = 416\n",
    "\n",
    "model = GPTTrainer(vocab_size, d_model, ilength, n_layers, d_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with any text file containing full set of data\n",
    "mozart_data = 'txt-files/notewise/custom/mozart.txt'\n",
    "\n",
    "with open(mozart_data, 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocabulary set\n",
    "vocab = sorted(tuple(set(text.split())))\n",
    "n = len(vocab)\n",
    "\n",
    "# create word-integer encoder/decoder\n",
    "word2int = dict(zip(vocab, list(range(n))))\n",
    "int2word = dict(zip(list(range(n)), vocab))\n",
    "\n",
    "# encode all words in dataset into integers\n",
    "encoded = np.array([word2int[word] for word in text.split()])\n",
    "\n",
    "# get vocab_size\n",
    "vocab_size=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into 90% train and 10% using index\n",
    "val_idx = int(len(encoded) * (1 - 0.1))\n",
    "train_data, val_data = encoded[:val_idx], encoded[val_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, seq_len):\n",
    "    \n",
    "    # trim data set to be multiple of seq_len\n",
    "    num_seq = len(dataset)//seq_len\n",
    "    trim_len = num_seq*seq_len\n",
    "    dataset = dataset[:trim_len]\n",
    "    \n",
    "    # reshape dataset into sequences\n",
    "    dataset = np.reshape(dataset, (num_seq, seq_len))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(dataset, batch_size):\n",
    "    indices = list(np.random.randint(0, dataset.shape[0], size=batch_size))\n",
    "    seq_len = dataset.shape[1]\n",
    "    \n",
    "    batch_output = dataset[indices,:]\n",
    "    batch_output_mask = np.ones((batch_size, seq_len), dtype=bool)\n",
    "    \n",
    "    return batch_output, batch_output_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Shape: (79853, 128), Test Set Shape: (8872, 128)\n"
     ]
    }
   ],
   "source": [
    "d_train = prepare_dataset(train_data, 128)\n",
    "d_valid = prepare_dataset(val_data, 128)\n",
    "\n",
    "print(\"Train Set Shape: {}, Test Set Shape: {}\".format(d_train.shape, d_valid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iteration 0, Train Loss: 14.469578742980957, Val Loss: 11.785544395446777\n",
      "Epoch 0 Iteration 50, Train Loss: 7.486458778381348, Val Loss: 7.417486667633057\n",
      "Epoch 0 Iteration 100, Train Loss: 7.582685470581055, Val Loss: 7.410603046417236\n",
      "Epoch 0 Iteration 150, Train Loss: 7.377954483032227, Val Loss: 7.195443153381348\n",
      "Epoch 0 Iteration 200, Train Loss: 6.044005393981934, Val Loss: 5.789759635925293\n",
      "Epoch 0 Iteration 250, Train Loss: 5.636773109436035, Val Loss: 5.298721790313721\n",
      "Epoch 0 Iteration 300, Train Loss: 5.255577564239502, Val Loss: 4.793505668640137\n",
      "Epoch 0 Iteration 350, Train Loss: 4.646119117736816, Val Loss: 4.2281107902526855\n",
      "Epoch 0 Iteration 400, Train Loss: 4.030297756195068, Val Loss: 3.804192543029785\n",
      "Epoch 0 Iteration 450, Train Loss: 3.8890271186828613, Val Loss: 3.352569818496704\n",
      "Epoch 0 Iteration 500, Train Loss: 3.4022698402404785, Val Loss: 3.048938035964966\n",
      "Epoch 0 Iteration 550, Train Loss: 3.040712356567383, Val Loss: 2.8604846000671387\n",
      "Epoch 0 Iteration 600, Train Loss: 2.956061363220215, Val Loss: 2.7747855186462402\n",
      "Epoch 1 Iteration 0, Train Loss: 2.8654773235321045, Val Loss: 2.7544291019439697\n",
      "Epoch 1 Iteration 50, Train Loss: 2.6730093955993652, Val Loss: 2.6243579387664795\n",
      "Epoch 1 Iteration 100, Train Loss: 2.649312973022461, Val Loss: 2.5464184284210205\n",
      "Epoch 1 Iteration 150, Train Loss: 2.4661078453063965, Val Loss: 2.4845023155212402\n",
      "Epoch 1 Iteration 200, Train Loss: 2.5454201698303223, Val Loss: 2.407672166824341\n",
      "Epoch 1 Iteration 250, Train Loss: 2.385016441345215, Val Loss: 2.366415023803711\n",
      "Epoch 1 Iteration 300, Train Loss: 2.368840217590332, Val Loss: 2.3094711303710938\n",
      "Epoch 1 Iteration 350, Train Loss: 2.3476147651672363, Val Loss: 2.2859015464782715\n",
      "Epoch 1 Iteration 400, Train Loss: 2.3553357124328613, Val Loss: 2.234617233276367\n",
      "Epoch 1 Iteration 450, Train Loss: 2.2300381660461426, Val Loss: 2.1979050636291504\n",
      "Epoch 1 Iteration 500, Train Loss: 2.2065606117248535, Val Loss: 2.1671669483184814\n",
      "Epoch 1 Iteration 550, Train Loss: 2.2690718173980713, Val Loss: 2.1435811519622803\n",
      "Epoch 1 Iteration 600, Train Loss: 2.1306395530700684, Val Loss: 2.107687473297119\n",
      "Epoch 2 Iteration 0, Train Loss: 2.185006618499756, Val Loss: 2.0528836250305176\n",
      "Epoch 2 Iteration 50, Train Loss: 1.9789164066314697, Val Loss: 2.029050827026367\n",
      "Epoch 2 Iteration 100, Train Loss: 2.0833685398101807, Val Loss: 1.992071270942688\n",
      "Epoch 2 Iteration 150, Train Loss: 1.98281991481781, Val Loss: 1.9793524742126465\n",
      "Epoch 2 Iteration 200, Train Loss: 2.1025772094726562, Val Loss: 1.9845927953720093\n",
      "Epoch 2 Iteration 250, Train Loss: 2.0455312728881836, Val Loss: 1.9353094100952148\n",
      "Epoch 2 Iteration 300, Train Loss: 2.0463497638702393, Val Loss: 1.944435477256775\n",
      "Epoch 2 Iteration 350, Train Loss: 1.9642868041992188, Val Loss: 1.909030795097351\n",
      "Epoch 2 Iteration 400, Train Loss: 1.8602304458618164, Val Loss: 1.893621563911438\n",
      "Epoch 2 Iteration 450, Train Loss: 1.9511830806732178, Val Loss: 1.8843857049942017\n",
      "Epoch 2 Iteration 500, Train Loss: 1.9557220935821533, Val Loss: 1.8677923679351807\n",
      "Epoch 2 Iteration 550, Train Loss: 1.9484819173812866, Val Loss: 1.853767991065979\n",
      "Epoch 2 Iteration 600, Train Loss: 1.9006798267364502, Val Loss: 1.8434405326843262\n",
      "Epoch 3 Iteration 0, Train Loss: 1.8284720182418823, Val Loss: 1.8339215517044067\n",
      "Epoch 3 Iteration 50, Train Loss: 1.9042637348175049, Val Loss: 1.8088445663452148\n",
      "Epoch 3 Iteration 100, Train Loss: 1.8937801122665405, Val Loss: 1.8123167753219604\n",
      "Epoch 3 Iteration 150, Train Loss: 1.7823823690414429, Val Loss: 1.7929877042770386\n",
      "Epoch 3 Iteration 200, Train Loss: 1.8832542896270752, Val Loss: 1.7879263162612915\n",
      "Epoch 3 Iteration 250, Train Loss: 1.8870984315872192, Val Loss: 1.7849661111831665\n",
      "Epoch 3 Iteration 300, Train Loss: 1.7711527347564697, Val Loss: 1.7718591690063477\n",
      "Epoch 3 Iteration 350, Train Loss: 1.831305980682373, Val Loss: 1.7542288303375244\n",
      "Epoch 3 Iteration 400, Train Loss: 1.8045499324798584, Val Loss: 1.7505099773406982\n",
      "Epoch 3 Iteration 450, Train Loss: 1.7133629322052002, Val Loss: 1.7409831285476685\n",
      "Epoch 3 Iteration 500, Train Loss: 1.7543036937713623, Val Loss: 1.7370538711547852\n",
      "Epoch 3 Iteration 550, Train Loss: 1.684226155281067, Val Loss: 1.7103397846221924\n",
      "Epoch 3 Iteration 600, Train Loss: 1.7258721590042114, Val Loss: 1.72843599319458\n",
      "Epoch 4 Iteration 0, Train Loss: 1.7084945440292358, Val Loss: 1.7210397720336914\n",
      "Epoch 4 Iteration 50, Train Loss: 1.7532697916030884, Val Loss: 1.7308768033981323\n",
      "Epoch 4 Iteration 100, Train Loss: 1.6191155910491943, Val Loss: 1.7183605432510376\n",
      "Epoch 4 Iteration 150, Train Loss: 1.6104037761688232, Val Loss: 1.7171337604522705\n",
      "Epoch 4 Iteration 200, Train Loss: 1.6861202716827393, Val Loss: 1.7017642259597778\n",
      "Epoch 4 Iteration 250, Train Loss: 1.5928099155426025, Val Loss: 1.6892794370651245\n",
      "Epoch 4 Iteration 300, Train Loss: 1.677180528640747, Val Loss: 1.6893082857131958\n",
      "Epoch 4 Iteration 350, Train Loss: 1.5993280410766602, Val Loss: 1.6852219104766846\n",
      "Epoch 4 Iteration 400, Train Loss: 1.7511096000671387, Val Loss: 1.6675816774368286\n",
      "Epoch 4 Iteration 450, Train Loss: 1.6716673374176025, Val Loss: 1.6725773811340332\n",
      "Epoch 4 Iteration 500, Train Loss: 1.5733381509780884, Val Loss: 1.6681503057479858\n",
      "Epoch 4 Iteration 550, Train Loss: 1.6268706321716309, Val Loss: 1.6576411724090576\n",
      "Epoch 4 Iteration 600, Train Loss: 1.6113991737365723, Val Loss: 1.6623177528381348\n",
      "Epoch 5 Iteration 0, Train Loss: 1.6360535621643066, Val Loss: 1.6326262950897217\n",
      "Epoch 5 Iteration 50, Train Loss: 1.5984165668487549, Val Loss: 1.6260796785354614\n",
      "Epoch 5 Iteration 100, Train Loss: 1.6031882762908936, Val Loss: 1.618804931640625\n",
      "Epoch 5 Iteration 150, Train Loss: 1.605672836303711, Val Loss: 1.604222059249878\n",
      "Epoch 5 Iteration 200, Train Loss: 1.5543065071105957, Val Loss: 1.6091580390930176\n",
      "Epoch 5 Iteration 250, Train Loss: 1.6066105365753174, Val Loss: 1.6166836023330688\n",
      "Epoch 5 Iteration 300, Train Loss: 1.6604888439178467, Val Loss: 1.5977582931518555\n",
      "Epoch 5 Iteration 350, Train Loss: 1.51378333568573, Val Loss: 1.5836715698242188\n",
      "Epoch 5 Iteration 400, Train Loss: 1.5976314544677734, Val Loss: 1.5721544027328491\n",
      "Epoch 5 Iteration 450, Train Loss: 1.5547091960906982, Val Loss: 1.5835702419281006\n",
      "Epoch 5 Iteration 500, Train Loss: 1.496321439743042, Val Loss: 1.5725769996643066\n",
      "Epoch 5 Iteration 550, Train Loss: 1.4325966835021973, Val Loss: 1.5869184732437134\n",
      "Epoch 5 Iteration 600, Train Loss: 1.5088260173797607, Val Loss: 1.575649380683899\n",
      "Epoch 6 Iteration 0, Train Loss: 1.4698970317840576, Val Loss: 1.5588456392288208\n",
      "Epoch 6 Iteration 50, Train Loss: 1.4801356792449951, Val Loss: 1.5538830757141113\n",
      "Epoch 6 Iteration 100, Train Loss: 1.481905221939087, Val Loss: 1.5455114841461182\n",
      "Epoch 6 Iteration 150, Train Loss: 1.4893627166748047, Val Loss: 1.546800971031189\n",
      "Epoch 6 Iteration 200, Train Loss: 1.4762595891952515, Val Loss: 1.5350931882858276\n",
      "Epoch 6 Iteration 250, Train Loss: 1.5241540670394897, Val Loss: 1.5340332984924316\n",
      "Epoch 6 Iteration 300, Train Loss: 1.4914731979370117, Val Loss: 1.5393520593643188\n",
      "Epoch 6 Iteration 350, Train Loss: 1.4242351055145264, Val Loss: 1.5338950157165527\n",
      "Epoch 6 Iteration 400, Train Loss: 1.462435007095337, Val Loss: 1.5242505073547363\n",
      "Epoch 6 Iteration 450, Train Loss: 1.462457537651062, Val Loss: 1.5293316841125488\n",
      "Epoch 6 Iteration 500, Train Loss: 1.5292069911956787, Val Loss: 1.5200115442276\n",
      "Epoch 6 Iteration 550, Train Loss: 1.4733836650848389, Val Loss: 1.5233073234558105\n",
      "Epoch 6 Iteration 600, Train Loss: 1.4885318279266357, Val Loss: 1.5149929523468018\n",
      "Epoch 7 Iteration 0, Train Loss: 1.4392532110214233, Val Loss: 1.502873420715332\n",
      "Epoch 7 Iteration 50, Train Loss: 1.4554443359375, Val Loss: 1.5045344829559326\n",
      "Epoch 7 Iteration 100, Train Loss: 1.420738697052002, Val Loss: 1.4899417161941528\n",
      "Epoch 7 Iteration 150, Train Loss: 1.4205049276351929, Val Loss: 1.4768487215042114\n",
      "Epoch 7 Iteration 200, Train Loss: 1.391059398651123, Val Loss: 1.4879693984985352\n",
      "Epoch 7 Iteration 250, Train Loss: 1.4336438179016113, Val Loss: 1.4815318584442139\n",
      "Epoch 7 Iteration 300, Train Loss: 1.3916200399398804, Val Loss: 1.4807713031768799\n",
      "Epoch 7 Iteration 350, Train Loss: 1.3883299827575684, Val Loss: 1.4798718690872192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Iteration 400, Train Loss: 1.396698236465454, Val Loss: 1.4821534156799316\n",
      "Epoch 7 Iteration 450, Train Loss: 1.4335107803344727, Val Loss: 1.4759868383407593\n",
      "Epoch 7 Iteration 500, Train Loss: 1.3463845252990723, Val Loss: 1.4627560377120972\n",
      "Epoch 7 Iteration 550, Train Loss: 1.4227793216705322, Val Loss: 1.478621244430542\n",
      "Epoch 7 Iteration 600, Train Loss: 1.3950715065002441, Val Loss: 1.4647849798202515\n",
      "Epoch 8 Iteration 0, Train Loss: 1.4105567932128906, Val Loss: 1.4455336332321167\n",
      "Epoch 8 Iteration 50, Train Loss: 1.3231462240219116, Val Loss: 1.452998399734497\n",
      "Epoch 8 Iteration 100, Train Loss: 1.380520224571228, Val Loss: 1.4401969909667969\n",
      "Epoch 8 Iteration 150, Train Loss: 1.3742295503616333, Val Loss: 1.4467943906784058\n",
      "Epoch 8 Iteration 200, Train Loss: 1.335031270980835, Val Loss: 1.443242073059082\n",
      "Epoch 8 Iteration 250, Train Loss: 1.3730876445770264, Val Loss: 1.4479875564575195\n",
      "Epoch 8 Iteration 300, Train Loss: 1.312730312347412, Val Loss: 1.4532997608184814\n",
      "Epoch 8 Iteration 350, Train Loss: 1.3250532150268555, Val Loss: 1.4468704462051392\n",
      "Epoch 8 Iteration 400, Train Loss: 1.4109995365142822, Val Loss: 1.4327393770217896\n",
      "Epoch 8 Iteration 450, Train Loss: 1.3526864051818848, Val Loss: 1.4322513341903687\n",
      "Epoch 8 Iteration 500, Train Loss: 1.3811273574829102, Val Loss: 1.4291338920593262\n",
      "Epoch 8 Iteration 550, Train Loss: 1.3091813325881958, Val Loss: 1.4326508045196533\n",
      "Epoch 8 Iteration 600, Train Loss: 1.3513672351837158, Val Loss: 1.4212353229522705\n"
     ]
    }
   ],
   "source": [
    "# Skeleton code, as in the previous notebook.\n",
    "# Write code training code and save your best performing model on the\n",
    "# validation set. We will be testing the loss on a held-out test dataset.\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # This is how you randomly initialize the Transformer weights.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    epochs = 20 #previously 50\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        batch_size = 128\n",
    "        iterations = d_train.shape[0] // batch_size\n",
    "        \n",
    "        # build validation set\n",
    "        e_output, e_output_mask = build_batch(d_valid, 200)\n",
    "        \n",
    "        for iteration in range(iterations):       \n",
    "\n",
    "            # Create a random mini-batch from the training dataset\n",
    "            batch_output, batch_output_mask = build_batch(d_train, batch_size)\n",
    "            # Build the feed-dict connecting placeholders and mini-batch\n",
    "            feed = {model.target_sequence: batch_output, model.decoder_mask: batch_output_mask}\n",
    "\n",
    "            # Obtain the loss. Be careful when you use the train_op and not, as previously.\n",
    "            train_loss, _, step = sess.run([model.loss, model.train_op, model.global_step], feed_dict=feed)\n",
    "            \n",
    "            if iteration % 50 == 0:\n",
    "                \n",
    "                # get validation loss\n",
    "                feed_val = {model.target_sequence: e_output, model.decoder_mask: e_output_mask}\n",
    "                valid_loss = sess.run(model.loss, feed_dict=feed_val)\n",
    "                \n",
    "                print(\"Epoch {} Iteration {}, Train Loss: {}, Val Loss: {}\".format(epoch, iteration, train_loss, valid_loss))\n",
    "                \n",
    "#                 print(\"Epoch {} Iteration {}, Train Loss: {}\".format(epoch, iteration, train_loss))\n",
    "                \n",
    "#                 This is how you save model weights into a file\n",
    "                model.saver.save(sess, root_folder+\"models/gpt_music\")    \n",
    "\n",
    "#                 # This is how you restore a model previously saved\n",
    "#                 model.saver.restore(sess, root_folder+\"models/transformer_summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the music generation gpt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the file path to your best performing model in the string below.\n",
    "\n",
    "model_file = root_folder+\"models/gpt_music\"\n",
    "# model_file = root_folder+\"models/transformer_summarizer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the validation loss of your model. We will use the code here with the unreleased test-set to evaluate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/gpt_music\n",
      "Validation loss: 1.41856\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    model.saver.restore(sess, model_file)\n",
    "\n",
    "    e_output, e_output_mask = build_batch(d_valid, 200)\n",
    "    feed = {model.target_sequence: e_output, model.decoder_mask: e_output_mask}\n",
    "    valid_loss = sess.run(model.loss, feed_dict=feed)\n",
    "    print(\"Validation loss:\", valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes significantly longer just taking the max\n",
    "\n",
    "def choose_top_words(arr, k):\n",
    "\n",
    "    # get top k indexes\n",
    "    # argsort in increasing order, get last k highest elements, reverse\n",
    "    top_k_words = np.flip(arr.argsort(axis=2)[:,:,-k:], axis=2)\n",
    "\n",
    "    # get corresponding logits sorted in decreasing order\n",
    "    top_k_logits = np.flip(np.sort(arr, axis=2)[:,:,-k:], axis=2)\n",
    "\n",
    "    # get sum of logits\n",
    "    logits_sum = top_k_logits.sum(axis=2)\n",
    "\n",
    "    # softmax top k logits into probabilities\n",
    "    p = top_k_logits / logits_sum[0][:, np.newaxis]\n",
    "\n",
    "    # hold cumulative distribution\n",
    "    c = p.cumsum(axis=2)\n",
    "\n",
    "    # generate random uniform samples\n",
    "    u = np.random.rand(1, len(p[0]), 1)\n",
    "\n",
    "    # get indexes of selected logits\n",
    "    choices = (u < c).argmax(axis=2)\n",
    "\n",
    "    # map selected indexes bacck to top_k_words\n",
    "    chosen_words = np.array([[top_k_words[0][i][j] for i, j in enumerate(choices[0])]])\n",
    "    \n",
    "    return chosen_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are generating one composition with our model. To evaluate it, we are going to generate a large number of them (100+) and then apply some statistics such as the BLEU score to estimate how well our model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/gpt_music\n",
      "The final summary:\n",
      "endp0 wait1 wait1 p23 p24 p40 p45 wait5 endp45 endp45 wait1 wait3 p42 p43 wait3 wait1 endp42 endp42 wait4 wait2 endp23 p45 endp24 wait5 wait1 endp45 p19 wait3 p47 p40 wait3 endp40 p47 endp47 wait5 p48 endp19 wait5 wait1 endp47 p19 p52 wait3 wait2 endp19 endp43 endp52 wait1 wait1 p50 p50 wait5 wait5 endp38 endp45 endp50 endp47 wait1 wait6 p48 p21 wait6 wait5 p48 endp45 endp48 wait2 wait2 endp21 p47 p47 wait2 wait3 endp47 p48 wait4 endp48 p47 wait1 endp47 endp21 wait2 wait5 endp47 p18 wait1 p42 p50 wait2 wait2 endp50 endp42 wait1 wait1 p48 p48 wait5 wait3 endp18 endp48 endp48 wait1 wait1 p24 p19 p45 p35 wait3 p47 p47 wait3 wait1 endp47 p48 wait5 wait1 endp45 p52 endp47 wait5 wait5 endp57 endp45 wait3 p52 p48 wait3 wait1 endp52\n"
     ]
    }
   ],
   "source": [
    "output_length = 128\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model.saver.restore(sess, model_file)\n",
    "\n",
    "    decoded_so_far = [0]\n",
    "    \n",
    "    for j in range(output_length):\n",
    "        padded_decoder_input, decoder_mask = pad_sequence(decoded_so_far, pad_index, output_length)\n",
    "        padded_decoder_input = [padded_decoder_input]\n",
    "        decoder_mask = [decoder_mask]\n",
    "#         print(\"========================\")\n",
    "#         print(padded_decoder_input)\n",
    "        # Use the model to find the distrbution over the vocabulary for the next word\n",
    "        feed = {model.target_sequence: padded_decoder_input,\n",
    "                model.decoder_mask: decoder_mask}\n",
    "        logits = sess.run([model.decoded_logits], feed_dict=feed)\n",
    "    \n",
    "#        chosen_words = np.argmax(logits[0], axis=2) # Take the argmax, getting the most likely next word\n",
    "        chosen_words = choose_top_words(logits[0], 3)\n",
    "        next_word = int(chosen_words[0, j])\n",
    "        decoded_so_far.append(next_word) # We add it to the summary so far\n",
    "\n",
    "\n",
    "print(\"The final summary:\")\n",
    "print(\" \".join([vocab[i] for i in decoded_so_far]).replace(\"▁\", \" \"))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
